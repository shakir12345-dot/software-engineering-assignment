Title:
   A Soft Alignment Model for Bug Deduplication

Authors names:
    1.Irving Muller Rodrigues 
    2.Daniel Aloise
    3.Eraldo Rezende Fernandes
    4.Michel Dagenais

where:
   In 17th International Conference on Mining Software Repositories (MSR '20), October
5-6, 2020, Seoul, Republic of Korea. ACM, New York, NY, USA
    
Introduction:
    Bug fixing accounts for a substantial part of any software development project. Thus, many projects make use of a bug tracking system (BTS) to manage and track bug reports. One important task in such systems is to identify duplicate bug reports, i.e., distinct reports describing issues caused by the same bug in the software.
It is crucial to perform this task as fast as possible in order to prevent developers from spending time looking for bugs already fixed. Usually, a triage team manually labels new reports as duplicate or not [29]. However, especially in open source projects, bug reports can be submitted by developers, testers and even end users. This
heterogeneous environment leads to many duplicate bug reports. For example, 12% of all reports are duplicate in one Eclipse instance [4]. Therefore, devising automatic methods to detect duplicate bug reports is crucial for efficient software maintenance. In the literature, such a task is called duplicate bug report detection, bug report
deduplication or, simply, bug deduplication. Typically, a bug report comprises a summary, a description, and some categorical fields (e.g., system, component and version). Regarding textual data, for simplicity, the terms word and token are considered interchangeable in this work. One core component of most methods in bug deduplication is a similarity function that compares a pair of reports. How this function is composed and used vary greatly from one method to another. A handful of studies [6, 10, 22, 37] employ deep neural networks in order to model similarity functions. Deshmukh et al. [10], Budhiraja et 
al. [6] and Xie et al. [37] works are based on Siamese neural networks [8] that generate the representation of one bug report without considering the other report content. This independent representation is limited
specially for textual data, since it may focus on generic features that are not relevant for a specific comparison [32]. Poddar et al. [22] try to mitigate that shortcoming by employing an architecture
that exchanges information between the reports during feature extraction. This approach generates a joint representation based on attention [1], in which the representation of a word in a report
attends to a pooled representation of all words in the other report. In this work, we propose a novel deep learning network that produces joint representations of reports based on a soft attention
alignment mechanism [20]. The key component of this model is a layer that compares each word in a report with a fixed length representation of all words in the other one. While Poddar et al. [22] also use an attention mechanism, our proposed architecture is able to summarize relevant information within one report conditioned to a specific segment of the other report. This provides a more powerful representation of textual data.
Many previous works on bug deduplication have employed an
evaluation methodology called decision making approach [17]. This evaluation is based on pairs of reports labeled as positive when they refer to the same bug or negative otherwise. Positive pairs comprise all possible pairs within a set of duplicate reports. While negative pairs are generated using some sampling technique. Model performance is then measured by means of ordinary classification metrics (like accuracy, precision and recall) over the generated set of positive and negative pairs. The decision making approach is quite unrealistic, since the real scenario presents a much larger set of negative candidates. When a new report is submitted to a BTS, all previously submitted reports are duplicate candidates. Thus, this evaluation methodology highly overestimate performance. Another popular evaluation methodology is the ranking approach. It acknowledges that the current techniques are not accurate enough to automatically detect duplicate reports with no human intervention. Therefore, in this approach, for a given new report, the proposed methods generate a list of the K most likely duplicate reports. The triager then identify whether a report is duplicate considering only
the reports from the recommendation list. Instead of searching and examining hundreds of reports in the BTS, the triager can focus on the K recommended reports.
We experimentally evaluate our model by means of a ranking methodology based on Sun et al. [30]. We report on experiments using four well known datasets derived from BTSs of open source projects, namely Eclipse, Mozilla, NetBeans and OpenOffice. Bug deduplication in open source projects is particularly challenging because any user can submit a bug report in their BTS and the
knowledge of these users about the system may vary significantly. State of the art systems and some strong baselines are compared to the proposed model in several scenarios. Additionally, we perform an ablation study to assess different aspects of our model.
The main contributions of this paper are summarized as follows:
(1) We propose a soft alignment model that is based on a more powerful architecture than previous methods.
(2) Our method and the baselines are evaluated using a more realistic methodology. This work is the first to compare different deep learning methods using the ranking approach.
(3) Our method achieves state of the art performance on all considered datasets.

 RELATED WORK:
    Several non deep learning methods in the literature address the bug deduplication as a ranking problem. Runeson et al. [26] are the first to use NLP techniques to approach duplicate bug report detection. They measure report similarity by computing the cosine similarity between bag of words vectors. Wang et al. [36] detected
duplicate reports by combining function calls during the system execution with textual data. Sun et al. [30] trained an SVM to estimate the probability of reports being duplicate by receiving 54 textual similarity features generated from different combinations
of text origins, n-gram lengths and dictionaries. Sureka and Jalote [31] proposed a similarity function whose output is proportional to the quantity of n-gram of characters in common between two reports. Sun et al. [29] proposed BM25Fext and REP for bug deduplication. BM25Fext is an extension of BM25F specially designed
to address scenarios in which queries are long sentences with few or no duplicate words. REP is a similarity function that linearly combines BM25Fext scores using unigram and bigram with features generated from categorical data comparisons. Prifti et al. [23] developed a method to rank reports using a time window and a unique representation for each master group. Nguyen et al. [19] proposed a method, called DBTM, that linearly combines the BM25F score and the topic similarity computed by a model based on Latent Dirichlet Allocation (LDA). Banerjee et al. [2] addressed the bug report deduplication by using the longest common subsequence between the bug reports. Zhou and Zhang [40] proposed a linear model, called BugSim, which is trained to minimize the fidelity loss of triplets using features inspired by Nallapati [18]. In Yang et al. [38], BM25 is used to weight the bag of words vectors which are compared by the cosine similarity. Banerjee et al. [3] generated a top 20 list for different similarity measures and aggregated them into a unique list using two fusion approaches: one retrieves the maximum score of a report in the lists while the other sums the similarity scores of the reports. Lerch and Mezini [15] proposed to use the stack trace in the bug report to better detect duplicate bug reports. Sabor et al. [27] improved Lerch and Mezini [15] by employing only packages names instead of full method names. Lin and Yang [16] combined TF-IDF with a weighting scheme based on the term relations within the clusters of reports. Lin et al. [17] trained an SVM to estimate the duplication probability using cluster based correlation features, the BM25F score and the cosine similarity of word vectors. Yang et al. [39] designed similarity function whose output depends on product and component differences, the cosine similarity of TF-IDF vectors and the average of word embeddings. Budhiraja et al. [7] proposed LWE which combines LDA with the word embeddings. Regarding deep learning methods, Xie et al. [37] proposed a convolutional neural network (CNN), called DBR-CNN, to classify pairs of duplicate bug reports. In their architecture, a shared CNN independently encodes the textual data of the pair of reports into two vectors. A logistic regression then classifies each pair of reports by receiving the cosine similarity of those vectors and a set of features related to categorical data. In NLP, statistical methods parse textual data from documents to discover latent themes, called topics, which are common between multiple documents [5] (e.g., bug reports that contain the words combo or font can be related to the topic UI). Poddar et al. [22] proposed a neural network that simultaneously learns to cluster reports based on topics while detecting duplicate pairs. A recurrent neural network (RNN) represents each word of a report as a vector. The k first dimensions of these vectors are trained to yield high similarities to words that are in the same topic. For the classification, Poddar et al. [22] generate the joint representation of a report as a weighted average of its word vectors. An attention mechanism calculates these weights by using the self attention coefficients of the topic information and the elementwise multiplication of the word representations in the report with the mean pooling of all words in the other report. The authors used only summary data from the reports in their experiments. Budhiraja et al. [6] proposed a neural network, called DWEN, in which the fixed length representation of a report is the mean of
their word vectors and the classifier is a multilayer neural network (MLNN) that receives only the representation of a pair of reports. Deshmukh et al. [10] proposed two siamese neural networks for bug deduplication. The authors used a feedforward neural network, a CNN, and a bidirectional LSTM to encode, respectively, the categorical data, the description, and the summary into vectors. The concatenation of these encoder outputs generates the fixedlength representation of the reports. Deshmukh et al. [10] proposed two approaches to calculate the similarity of the report representations. The first one, called Siamese Triplet, is trained to minimize a hinge loss given a set of triplets and employs the cosine function to compute the similarity between two reports. The second one, called Siamese Pair, uses the binary cross entropy loss of pair in the training and scores the similarity between reports using a MLNN. This paper presents a method that improves the representation generation found in the previous deep learning approaches. Different from Budhiraja et al. [6], Deshmukh et al. [10], and Xie et al. [37], our model exchanges information between the reports before encoding textual data into a fixed length vector. Moreover, in the feature extraction, our method can dynamically focus on different segments of a report instead of providing a unique set of features from it, as done by Poddar et al. [22]. This more powerful architecture can reduce information loss in the representation generation thereby improving the duplicate bug report detection.

SOFT ALIGNMENT MODEL FOR BUG
DEDUPLICATION:
    In this section, we describe our proposed Soft Alignment Model for Bug Deduplication (SABD). This model receives a pair of bug reports: a new query report q and a candidate reportc previously submitted to the repository. The model outputs the probability P(y|q,c) of q being a duplicate of c, where y indicates whether the given reports are duplicate (y = 1) or not (y = 0). We consider a bug report to be composed of the categorical fields, a summary and a description. Given a query report q, the values of its categorical fields are
represented as the tuple q cat while the sequence of words of its summary and description are denoted as q
s and q d, respectively. The same notation is employed for the candidate c. As we can see, SABD is composed of the categorical and textual modules (two sub-networks)
that independently compare the categorical and textual data from both reports, respectively. The classifier receives these module outputs and produces the final prediction P(y|q,c). While the categorical module is a straightforward dense neural network, a more sophisticated architecture is employed by the textual module to handle text. The core of this module is the soft alignment comparison layer that allows the model to dynamically access distinct information from the text. This mechanism is expected to improve the model capacity to focus on relevant features in the textual data
for the deduplication. In the remainder, we describe the details of SABD and its modules.

 Categorical Module:
    The categorical module is composed of three layers: embedding,encoder, and comparison. In the embedding layer, each categorical field is related to a parameterized lookup table that links the field value to a real-valued vector. This representation is more powerful than using binary variables (e.g., feature is 1 if and only if field values are equal) since it allows the model to group similar field values. Given the query q, the embedding layer concatenates the real-valued vectors of each categorical value in q and outputs eq ? Rclúdcat , where cl is the number of categorical fields in the report and dcat is a hyperparameter indicating the categorical vector dimensions. The encoder layer receives the embedding layer output eq and generates a fixed representation aq of the categorical data from q such that:
            aq = ReLU (W aeq + ba),

where W a ? Rdax(cl údcat )is the weight matrix parameter, b a ?Rda is the bias parameter, and da is a hyperparameter that controls the layer size. Analogously, the fixed representation ac is produced
for the categorical data of candidate c.

After encoding the categorical features into vectors aq and ac,
the categorical comparison layer computes a comparative representation of these two vectors by a simple operation given by:
                             cmp(aq, ac) = [(aq ? ac)2; aq ? ac],

where [ ú ; . . . ; ú ] is the concatenation operator and ? represents
element-wise multiplication. Finally, given the comparative representation, a fully connected (FC) layer computes the comparison
layer output as:

                           hcat = ReLU (W h[aq; ac; cmp(xcq, xcc )] + bh),

where Wh ? Rdh x4da is the FC weight matrix, bh ? Rdh is the FC bias vector, and dh is a hyperparameter.

Textual Module:
    Although categorical features are relevant to solving bug deduplication, the most informative features are the summary and description texts. Thus, the core of our model is the textual module that compares the textual features of the query and candidate reports (i.e.,qs, qd, cs and cd). It comprises four layers: textual embedding, soft alignment comparison, textual encoder, and textual comparison.

EXPERIMENTAL SETUP:
    In this section, we describe the main steps of our experimental setup the evaluation methodology, training procedure, used datasets, and competing methods. The data used in this work and the developed code are freely available 1.

Evaluation Methodology:
    Towards a more realistic evaluation setup than those used by previous deep learning methods, we evaluate our models using a rankingbased methodology similar to Sun et al. [29]. First, we sort the bug reports in a BTS by their creation date. Then, the reports are chronologically read and inserted in the training set until a specific date t.All the subsequent reports are used to create the test set. Finally, we group the reports in the training set that describe the same bug into buckets. In each bucket, the first submitted report is considered the master report and the remaining ones are the duplicate reports.
    In Table 1, we exemplify a BTS with five bug reports. Considering that t is 21/12/2018, we generate a training set composed of R1,R2, and R3 and a test set consisting of R4 and R5. The training set thus comprises two buckets: B1 = {R1, R3} and B2 = {R2}. During evaluation, we chronologically pick each report r in the test set.
    When r is a duplicate bug report, we generate a ranked list of the
buckets in the system. In this work, the score of a bucket Bi
is the
highest score yielded by a method when it compares r with each
report in Bi
. After checking whether r is duplicate, we consider
it as submitted and insert r into its correct bucket. Following this
procedure, for example, we first pick the report R4 in the scenario
of Table 1. A ranked list is not produced because R4 is not duplicate
and a new bucket B3 = {R4} is created. After that, the next report R5
is selected. Since it is duplicate, we generate a ranked list composed of B1, B2, and B3. Then, R5 is inserted in B1.
    Regarding the evaluation methodology used by other ranking approach methods, Budhiraja et al. [6] do not describe how the test dataset was generated nor the procedure to create the ranked list.Both are crucial elements of the evaluation methodology and can considerably impact the achieved performances. Deshmukh et al.[10] extract pairs of bug reports from a BTS and randomly split them into training and test datasets. In their evaluation, for each duplicate bug report, their method outputs a recommendation list composed of only reports within the test set. This artificially reduces the number of reports that must be searched for each queried report,which makes the problem much easier [4]. Furthermore, in the BTSs, we only have access to data that was reported before a current time x. Thus, the model can only be trained using data from this period. After training a model, it only examines bug reports that were created after x. Randomly shuffling the data allows the model to be trained with reports created from the future (after x) and to retrieve candidates that were submitted after the queried report.Additionally, this randomization makes the problem easier because it spreads more uniformly the features through the dataset and can mitigate the concept drift issue. Therefore, we believe our experimental setting is more realistic. We compare our methods with those proposed in Budhiraja et al. [6] and Deshmukh et al. [10].However, due to the methodological differences aforementioned,and since source code was not provided by authors, we implemented those methods to the best of our knowledge, as described in Section4.5.
    Like Sun et al. [29], we evaluate a method using two metrics: mean average precision (MAP) and recall rate@k (RR@k). Both metrics are based on the ranking of reports according to the scores computed by a method. MAP is a general ranking metric. In our setting, rankings only need to contain one relevant item per query to be considered a hit. Thus, MAP can be simplified as:
    whereQ is the number of duplicate bug reports in the evaluation set and pi is the position of the correct bucket in the ranked list. RR@k is equal to the ratio of duplicate reports whose correct buckets are within the top-k buckets in the given ranking to the number of duplicate bug reports. RR@k is defined as:
                   RR@k =nkQ,
    where nk is the number of query reports in the test set for which the corresponding bucket appears in the top-k positions of the ranking computed by a method.

EXPERIMENTAL RESULTS:
    Since the competing methods and SABD are stochatics, we perform five distinct runs for each experimental configuration6. We report in this section average performance in terms of MAP and RR@k, as well as standard deviations illustrated as error bands in figures and inside brackets in tables. Following Sun et al. [29], the RR@k is calculated for each k = 1, 2, . . . , 20. It is important to notice that, when evaluating a model, we include duplicate reports whose buckets are not within the considered time window. These duplicate reports are considered misses for RR@k computation, and their terms 1pi in the MAP expression are 0.
    
        




   